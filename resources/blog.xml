<?xml version="1.0" encoding="UTF-8"?>
<blog>
	<topic isoValue="20250219">
		<link>risk-management</link>
		<icon>bi bi-exclamation-triangle</icon>
		<title>Risk Management</title>
		<date>February 19, 2025</date>
		<pre>
			Part of any testing activity when working on projects involve managing risks potentially caused during planning and development. This means risk identification and assessment are some of the considerations in prioritizing which features are built, tested, and patched.
		</pre>
		<main>
			Identifying risks happens throughout the planning and delivery phases. Utilizing a combination of requirements gathering, user flows, domain knowledge, prior experiences, code changes, and stakeholder collaborations, testers can identify risk areas and issues. This provides a basis for where testing can focus on.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Next, assessing risks involve determining how likely each item can occur and its business impact. Obviously, the frequency of use and the visibility of the feature are proportional to the likelihood of occurrence. Other factors like loss of business, liability, damage to reputation, or safety concerns can greatly impact the level of risk. Sometimes, having workarounds for certain issues can decrease risk, on the other hand.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Lastly, classification of each risk item with its risk assessment can look different across organizations. Assignments like a numerical scale (1-5), categories (low, medium, high), or colours are utilized. Then, the team can finally prioritize, based on the overall ratings, any mitigations needed to manage and reduce the risks involved.
		</main>
	</topic>
	<topic isoValue="20250212">
		<link>entry-exit-criteria</link>
		<icon>bi bi-box-arrow-in-right</icon>
		<title>Entry &amp; Exit Criteria</title>
		<date>Feburary 12, 2025</date>
		<pre>
			When should test activities start and be completed? We will look closely at the importance of defining them to sustain continuing product development successes.
		</pre>
		<main>
			Entry and exit criteria related to testing may or may not be the same as the Definition of Ready or the Definition of Done in agile development, as most people imagine. The start and the end of testing activities could directly impact the start and the end of project delivery, and vice versa.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Ideally, there will be different entry criteria for various testing activities throughout the planning and delivery phases. Requirement verification can happen once the idealization of user stories is underway. Next, planning test cases will take place afterwards during implementation. Simultaneously, other test criteria like static checks can be added while incremental development work is going on. Finally, the availabilities of test environment and test data would be entry criteria for functional validation.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			The exit criteria for testing are more complex. The team should think about whether the planned tests have been executed, the requirements coverage has been achieved, the number of remaining unresolved defects is within a low limit and a tolerable risk, and a general level of quality has been attained. At times for certain projects that are smaller in scope, when the exit criteria are met, usually it means the definition of done is complete for delivery. For others, there may be other activities required for release, outside of engineering efforts.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			When these entry and exit criteria are not met or delayed, testing activities, and subsequently the project, will be more difficult to execute, more time will be spent, more costly resources will be utilized, and there will be more risk. Often, there are instances when testing activities start and end when the entry and exit criteria are not met because there is pressure to deliver. Proper communication during the development lifecycle needs to be addressed periodically amongst stakeholders to ensure there is a common understanding of risks.
		</main>
	</topic>
	<topic isoValue="20250205">
		<link>color-testing</link>
		<icon>bi bi-box</icon>
		<title>Colors of Testing</title>
		<date>February 5, 2025</date>
		<pre>
			As many of you know, I launched my blog, "Pitch Black With Jack", late last year ~ jacklinto.github.io/blog. Bookmark it on your browser, share it with your network, comment on it with your thoughts, or whatever strikes your fancy. Thank you for your support! It has been great reiterating my experiences in testing digitally and gives me a chance to evolve how I ponder about testing going forward too.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			A dear subject matter that was the namesake for my blog, this new post is about my views on the colors of testing.
		</pre>
		<main>
			Thinking back on when I tried to come up with ideas about naming this blog, I mainly thought about how I approached testing throughout my career (and some clever rhymes). The result is one of the colors and categories of test techniques that I still adhere to this day: black-box testing. One of the goals of this blog is to show that testing is a lot like a pitch-black journey. Every project you work on is complex, and sometimes you will have to feel blindly around to get through to the end of it.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Black-box testing is about user behaviour. Based on product specifications (and their testing experiences), testers derive test cases and test data to validate that there are no gaps between the requirements and the implemented product. At the same time, ensuring that we have the right product that users would be able to perform their critical workflows. Ideally, those requirements are complete and correct before implementation. More often than not, testers will have to execute testing based on inadequate information and limited time.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Consider the inputs and the outputs. What general and edge use cases are possible? The tester's skill, prior experiences, and intuition can also play a part in determining where errors can occur in a product. Moreover, informal exploratory testing further enhances validation on how users perceive features. However, testing techniques merely based on user behaviours is not enough.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			The other test technique is white-box testing that is based on the internal structure of the system. Collaborating with engineers, testers derive test cases and test data from learning about the code, architecture, and design of the feature components. These can include unit code tests, component integration tests, and static architecture and algorithm design reviews.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Most projects consist of both black-box and white-box testing techniques. There are others that apply a combination of both: grey-box testing. With knowledge of internal structures, dynamic code analysis like vulnerability scanning can be helpful for user flows.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			In the end, all of these techniques have their own places when testing. Some are domains of engineers or testers, and everyone in between. No matter the color of testing, it is important to iterate that testing is messy and complex.
		</main>
	</topic>
	<topic isoValue="20250129">
		<link>psychology-testing</link>
		<icon>bi bi-person-circle</icon>
		<title>Psychology of Testing</title>
		<date>January 29, 2025</date>
		<pre>
			Psychology of Testing<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			In our last post about politics of testing, we realized that there are some human psychological factors at play in activities relating to quality. It is worth another post to examine that further. Warning: possible trigger alert.
		</pre>
		<main>
			The underlying human trait for everyone involved in designing and building a product is bias. Product managers, developers, and designers have confirmation bias that can potentially make them harder to accept their ideas, code, or design as incorrect or flawed. On the other hand, pessimistic bias from testers may lead to negative skepticism of the produced work. Both can derail deliveries of projects because there could be under- or over-estimations of testing required.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Along the same lines, there is also bias for and against reporting defects. At times, quality metrics directly impact their roles. Testers typically report all issues to prove their worth in assessing their effectiveness. Conversely, developers may be likely to minimize reporting to decrease rework and ensure timely delivery. Ultimately, the right balance should be striked to ensured there is psychological safety for everyone to properly report issues, and job performance is not evaluated based on quality alone.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Being the messenger of bad news (and defects), inevitably, testers may be put into an unenviable position. Tensions arise as project progress is impacted. So, testing could be considered as a destructive activity. Logically, bias of testers versus others would results in different estimates of test resourcing and executions. To combat this, timely communication and constructive collaboration are keys to project success.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Furthermore, thinking about testing involves a combination of curiosity, problem-solving, attention to detail, diplomacy, and communication. These traits are wonderful qualities that everyone on the team should strive to upskill. The bias is not to focus on some of them but to seek to gain knowledge and skills in each domain every day.
		</main>
	</topic>
	<topic isoValue="20250122">
		<link>politics-testing</link>
		<icon>bi bi-people</icon>
		<title>Politics of Testing</title>
		<date>January 22, 2025</date>
		<pre>
			Politics of Testing<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			This post, also topical, dives into the political aspects of testing and makes an apt comparison to a career that personifies the role of a tester.
		</pre>
		<main>
			Testers do a lot of different things throughout the course of a business day. Of course, test planning and test execution take up the majority of that time. But implicitly, there are also the underlying interpersonal and personal aspects of what they do too. We want to speak to the interpersonal and the political parts here.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Unspokenly and understandably, there is always a gulf between testers and other parties. Finding defects can be perceived as criticism. We can dive deeper into the psychology of this in the next post. This tension can lead to people regarding testing as a destructive activity. While not entirely up to them, testers play a big part in reducing these misconstrued attitudes to more constructive ones.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			In a lot of ways, the role of tester is a lot like a diplomat. Not unlike the government officials that represent and protect the interests of their state, government, institution, or organizations with others, testers represent and protect the interests (quality) of the platform. To do that job well, they need to build good working relationships with other parties. The foundations of those connections are as follows:<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Collaboration is key when working with other functions. Rather than being adversaries for finding issues, testers are partners in delivering a project on time and with quality. Everyone should have the same common goals.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Secondly, the proper and timely communication of test results is important for the team. Being objective and based on facts goes a long way in building good will and trust for testers. Criticizing and being judgmental do not help in creating a collaborative environment.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Above all, the advocacy of quality within the team is also essential in promoting the benefits of and managing expectations of testing. The ability for testers to navigate different priorities and negotiate trade-offs will have large impacts to the success of a project. They need to ensure the other partners are aligned on what can be achieved within a timeline so that the team can make adjustments.
		</main>
	</topic>
	<topic isoValue="20241204">
		<link>testing-principle-7</link>
		<icon>bi bi-slash-circle</icon>
		<title>Testing Principle #7 - Absence-of-errors is a fallacy</title>
		<date>December 4, 2024</date>
		<pre>
			Absence-of-errors is a fallacy<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Take a moment to check our first two principles: https://jacklinto.github.io/blog.html#testing-principle-1 and https://jacklinto.github.io/blog.html#testing-principle-2, we now understand finding all the defects in a platform is impossible. Also, while we can find and fix those defects, does it mean that we have guaranteed ourselves the 'perfect' system? This is the last of the seven testing principles, aiming to dispel this myth.
		</pre>
		<main>
			This is the seventh principal listed in Section 1.3 of the ISTQB foundational level syllabus. Despite following, with the best of intentions, all the previous testing principles during the course of a project, there is no guarantee that we can achieve project success. This is because, even with lots of test planning and execution to uncover and remediate defects, there are still a lot of unknowns when we launch the project to users.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			One of the unknowns is the platform requirements. We have verified and validated that all the features satisfy the product requirements. But if the requirements were flawed or missing key features, we have the wrong or incomplete product. Sometimes, the documentation or the way it was marketed or positioned to be sold was unclear. Until we launch in production, we do not fully comprehend how users will use and perceive the platform, which may not be what we have hypothesized in the original requirements.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			The other obvious one is additional issues found only in production. We may not have accounted for specific environment configurations that would have different data sets, user profiles, infrastructure constraints, or unforeseen operations. It is important for the teams to learn from them to understand how they can be reproduced internally with some form of root cause analysis.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Lastly, there are external circumstances that cannot be controlled. The competition could release another competing product that may be easier to use or cheaper. New laws or regulations could pass that hinder the platform from reaching its potential. Market tastes could have shifted when we least expected. Support from partners and client champions could have moved on. All of these examples may render our product less than perfect.
		</main>
	</topic>
	<topic isoValue="20241127">
		<link>testing-principle-6</link>
		<icon>bi bi-asterisk</icon>
		<title>Testing Principle #6 - Testing is Context Dependent</title>
		<date>November 27, 2024</date>
		<pre>
			Testing is Context Dependent<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Just like a journalist asks the Five Ws to write a story, a tester will ask the same questions to determine their testing strategies. Follow along to check out what they are in this penultimate installment of the seven testing principles.
		</pre>
		<main>
			This is the sixth principal listed in Section 1.3 of the ISTQB foundational level syllabus. What do testers ask to come up with their testing strategies?<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Who are the audience or customers? Testers take on the persona of who will be using the product. They will generate specific test scenarios based on the problems they need to solve. Also, the type of testing artifacts or documents produced by testers will look different if it is external customers and customer-facing support teams vs. other testers vs. developers and product managers. We need to consider the levels of details, levels of technical expertise, user/role permissions, and levels of confidentiality or authorization.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			What is the industry being considered? The type of testing will look different depending on how much oversight and regulations need to be followed and how much risk needs to be considered. For example, testing in the aerospace and medical industries is not alike to testing in finance or even social media. With more compliance and risk-averse industries, the speed of testing slows down and is more regulated. On the other hand, the same slow approach cannot be utilized for the latter category due to the danger of falling behind other fast-paced competitors.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			When will the testing be scheduled? Are we following agile or waterfall methodology of development? Should we shift-left or do continuous testing? Given the other factors in this list, verification and validation activities can be executed differently at different times during the software lifecycle.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Where are testing happening? They can take place in internal environments or externally in production. Also, we will have to consider whether user acceptance testing needs to take place separately. Each of them comes with more decisions about which levels of data are utilized during testing, versioning, etc.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Why do we need testing? The successful outcomes of a project tell us the testing we need to perform. The insurance software should be able to predict risks based on financial records. The video streaming platform should be able to scale comfortably for millions of current users. These tell us what type of functional, performance, load, security (and others) testing is necessary.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			How is the product being developed? The type of testing will look different based on architecture decisions, service or API solutions, and whether it is intended as hardware products. While the programming languages or protocols used may not matter, they also provide an insight on the areas that engineers are focusing on to fill in security gaps or over-engineer data shortcomings.
		</main>
	</topic>
	<topic isoValue="20241120">
		<link>testing-principle-5</link>
		<icon>bi bi-repeat</icon>
		<title>Testing Principle #5 - Beware of the Pesticide Paradox</title>
		<date>November 20, 2024</date>
		<pre>
			Beware of the Pesticide Paradox<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			The continuous misuse of the same pesticides or antibiotics make them resistant to infections they are supposed to fighting against. The same can be said for executing the same tests repeatedly. We will examine what we can do about it in this next installment of the seven testing principles.
		</pre>
		<main>
			This is the fifth principal listed in Section 1.3 of the ISTQB foundational level syllabus. Continually running the same tests can prove to be futile because we may not find new bugs. In particular, remember that our first principle (check that previous post) still applies; our expectations cannot be too optimistic when we repeat ourselves. This is not to say we should not run them - we definitely should, but with some twists.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Updating test cases periodically is fundamentally important. This means setting up a frequency by which test cases are reviewed and updated with the most current behaviours and information. Furthermore, when new features are developed, updating existing test cases should be the default instead of creating new ones. This is essential to keep tests to a manageable state.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Another good way to keep tests fresh is to have different testers on the team execute them. This means setting up a frequency by which test case executions are rotated among team members. On one hand, since everyone thinks differently, how test steps are executed can vary, and therefore, tests can be updated accordingly with a variety of scenarios to cover. On the other hand, it facilitates knowledge sharing and upskills the team to be subject matter experts.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Lastly, implementing test automation can be a good investment if these tests need to be run consistently, like regression test cases. Sometimes, new bugs can even appear when tests are run repeatedly this way. In the end, it is not enough to just have tests - we must be able to refresh them whenever there is a need, since the platform under testing is never static.
		</main>
	</topic>
	<topic isoValue="20241113">
		<link>testing-principle-4</link>
		<icon>bi bi-arrows-angle-contract</icon>
		<title>Testing Principle #4 - Defects Cluster Together</title>
		<date>November 13, 2024</date>
		<pre>
			Defects cluster together<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Bugs in real life and in software are social creatures; they usually congregate in the same small (tight, dark) crevices of buildings and system modules. We shall dig deeper into this phenomenon in the fourth installment of the seven testing principles.
		</pre>
		<main>
			This is the fourth principal listed in Section 1.3 of the ISTQB foundational level syllabus. Typically, defects are found during testing in small clusters of the platform. This runs counter to most statistical models out there, with even probability outcomes, because software development is a whole different animal. Defects appear not randomly but are distributed across certain modules in the platform since they are not alike.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Understandably, modules in a system are not all treated the same. Some are simple because they are static and have trivial workflows. While others are complex because they depend on widely disparate services, many developers worked on them, and have complicated business logics. The modules in the latter category take more time to plan and develop which have a higher likelihood of failures. Conversely, sometimes modules in the former category may have less attention devoted to them during planning and development or be executed more quickly with less oversight and inexperienced personnel. Finally, mission-critical functionalities can attract more interest. There are other factors that can contribute to where defects are found since dealing with software is always messy.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			This is why the previously mentioned risk analysis is important to assess which modules are worth more testing focus. That task takes the probabities out of verification and validation so that we can take an analytical approach in determining which and when certain test cases are executed for each risk area.
		</main>
	</topic>
	<topic isoValue="20241106">
		<link>testing-principle-3</link>
		<icon>bi bi-arrow-left-circle</icon>
		<title>Testing Principle #3 - Early Testing Saves Time and Money</title>
		<date>November 6, 2024</date>
		<pre>
			Early testing saves time and money.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			This is pretty intuitive and is agreed upon by everyone as a sound testing strategy. Executing some forms of testing earlier in the software development lifecycle is also called "shift left". We will look closer at this concept and understand the degree of ease in putting this into practice.
		</pre>
		<main>
			This is the third principal listed in Section 1.3 of the ISTQB foundational level syllabus. In order to have effective testing (finding defects, for example) in a project, some forms of verification and validation need to be executed as early as possible in the software development lifecycle. When viewing a typical diagram of the SDLC diagram, reading from left to right, it means shifting testing more towards the left direction, "shift left". This is especially valuable in reducing the length and the cost of the project. The cost of fixing issues is typically lower during the planning and development phases because the stakeholders are actively working on it already. The earlier defects are found and resolved, the earlier we can arrive at a stable version of the platform for release.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			As I mentioned earlier in a previous post about verification and validation testing, they can occur at all phases of a project or SDLC. The emphasis is on planning (and also being flexible enough) to apply the right approaches at the appropriate time to be save time and money properly. It can include requirement reviews during planning, static scans during development, and functional testing against requirements implemented, etc. This is called "continuous testing", a larger topic we can revisit more in a later post.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Testing early is something we all strive to achieve in a perfect world. However, due to budget and deadline constraints, that is not always possible. So we need to work with stakeholders to communicate the various testing strategies, however much that can be applied, used at each step of the way. Given the amount of resources, we may encounter that certain testing coverage may not be achieved or even be able to be executed until later. This means specific types of defects may not be uncovered until another dependency is done. That realistic perspective of testing is important to the success of a project.
		</main>
	</topic>
	<topic isoValue="20241030">
		<link>testing-principle-2</link>
		<icon>bi bi-bicycle</icon>
		<title>Testing Principle #2 - Exhaustive Testing is Impossible</title>
		<date>October 30, 2024</date>
		<pre>
			Exhaustive testing is impossible.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Nor do we want to exhaust ourselves executing an infinite number of test scenarios. There are more valuable ways to focus our testing efforts. We shall explore some of them in this second installment of the seven testing principles.
		</pre>
		<main>
			This is the second principal listed in Section 1.3 of the ISTQB foundational level syllabus. Testing every combination of inputs and outputs for any platform is not realistic except for trivial cases. That number could be infinite or close to it. If we have unlimited resources and time, we can come close to achieving total coverage. Since there is always an imminent deadline ahead, we should pick those test scenarios by focusing on risk analysis, test techniques, and priorities.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Risk analysis involves stakeholders identifying risk items in the platform and assessing their risk levels. Based on this analysis, we can determine which and when certain test cases are executed for each risk item. This is valuable because each business can determine their own levels of exposure to risks and test accordingly.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Test techniques mean some forms of verification and validation to execute. As mentioned before, the many layers of testing, when chosen appropriately, can greatly optimize testing for teams.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Priorities can be established by different factors: requirements and time. Based on the priority of user requirements, we can prioritize which and when certain test cases are executed for each specification. This is valuable because we are testing from the perspective and the importance of each end user scenario. The concept of time will be expanded on more in the next post.
		</main>
	</topic>
	<topic isoValue="20241023">
		<link>testing-principle-1</link>
		<icon>bi bi-bug</icon>
		<title>Testing Principle #1 - Presence of Defects</title>
		<date>October 23, 2024</date>
		<pre>
			Testing shows the presence of defects, not their absence.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			There is no correlation here? This is the paradoxical nature of testing and the perception of testing. We will dive into each of the seven testing principles in the following weeks, and this is the first one relating to the value of testing.
		</pre>
		<main>
			This is the first principal listed in Section 1.3 of the ISTQB foundational level syllabus. Testing reduces the likelihood of defects that are not found in a product. However, it does not mean that the product are both verified and validated 100% correctly, even if no defects remain. Most of the time, provided that proper testing processes were followed, those defects found remaining after all testing activities have been completed are usually lower in severity and priority. I have not seen any team that implemented a testing process that can find all the defects. It is not all doom and gloom; this is fine and realistic.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Therein lies caveat #1: some forms of verification and validation were executed. We are not going to go through what those are, since there are many layers of testing involved for different type of projects. One of the goals for testing is to proactively find and fix defects before changes are released or deployed.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			The other caveat #2: people have the right expectations. More importantly, we need to ensure stakeholders, such as leadership and customers, are aware that the team, to the best of their ability and given the contextual information they know at the time, developed and tested the feature requirements they have implemented. Despite all that, the end results still do not guarantee that there will not be defects, even if testing metrics tell a story otherwise.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Ultimately, there is some risk to all of this, but testing is still worth the effort to be successful in a project. Regardless of how much (limited or else) time the product engineering team can dedicate to testing, do it - it is better to be safe out there.
		</main>
	</topic>
	<topic isoValue="20241016">
		<link>verification-validation</link>
		<icon>bi bi-search</icon>
		<title>How do Verification and Validation Differ?</title>
		<date>October 16, 2024</date>
		<pre>
			I see "verify" and "validate" used strictly in certain ways or interchangeably during testing, is this a big deal? We will dig through the difference, application using an example, and some thoughts on their usages in this first installation of my new blog.
		</pre>
		<main>
			According to the glossary of ISTQB, they are defined as follows:<![CDATA[<br/>]]>
			Verification: confirmation by examination and through provision of objective evidence that specified requirements have been fulfilled.<![CDATA[<br/>]]>
			Validation: confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			At first glance, they look the same; but the main difference between the two is the end goal of the testing. We verify requirements of an application and we validate the intended use of the application. Let us imagine a form submission page. We verify all the fields are inputs and able to submit correctly. We validate that it meets the needs of the actual external and internal users: submission and data storage/application. These are strictly based on the definitions.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			There are many resources out there that claim certain activities as largely verification or validation testing. That is all fine because there are truths in there. However, to treat one or the other as simply as: static vs. dynamic, conducted before vs. after the development process, low-level vs. high-level testing, focusing on specs vs. products, white vs. black-box testing, which team is responsible for which, performed manually or by automation, requires code execution or not, are all beside the point.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Generally, testing activities are not rigid in most of SaaS. There are verification and validation testing occurring all the time during the course of planning and executing a project. This is why being agile in testing is also essential these days; product engineering teams are quickly iterating on new ideas every moment - they have to triple ensure they are delivering the correct feature requirements but also proving they are providing the right value for customers in everything they do, day in day out.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			In coming posts, I want to drive home the idea that software testing is messy, as with term definitions and how they are used in the real world. I mean that in a good way.
		</main>
	</topic>
</blog>