<?xml version="1.0" encoding="UTF-8"?>
<blog>
	<topic isoValue="20241030">
		<link>testing-principle-2</link>
		<icon>bi bi-bicycle</icon>
		<title>Testing Principle #2 - Exhaustive Testing is Impossible</title>
		<date>October 30, 2024</date>
		<pre>
			Exhaustive testing is impossible.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Nor do we want to exhaust ourselves executing an infinite number of test scenarios. There are more valuable ways to focus our testing efforts. We shall explore some of them in this second installment of the seven testing principles.
		</pre>
		<main>
			This is the second principal listed in section 1.3 of the ISTQB foundational level syllabus. Testing every combination of inputs and outputs for any platform is not realistic except for trivial cases. That number could be infinite or close to it. If we have unlimited resources and time, we can come close to achieving total coverage. Since there is always an imminent deadline ahead, we should pick those test scenarios by focusing on risk analysis, test techniques, and priorities.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Risk analysis involves stakeholders identifying risk items in the platform and assessing their risk levels. Based on this analysis, we can determine which and when certain test cases are executed for each risk item. This is valuable because each business can determine their own levels of exposure to risks and test accordingly.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Test techniques mean some forms of verification and validation to execute. As mentioned before, the many layers of testing, when chosen appropriately, can greatly optimize testing for teams.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Priorities can be established by different factors: requirements and time. Based on the priority of user requirements, we can prioritize which and when certain test cases are executed for each specification. This is valuable because we are testing from the perspective and the importance of each end user scenario. The concept of time will be expanded on more in the next post.
		</main>
	</topic>
	<topic isoValue="20241023">
		<link>testing-principle-1</link>
		<icon>bi bi-bug</icon>
		<title>Testing Principle #1 - Presence of Defects</title>
		<date>October 23, 2024</date>
		<pre>
			Testing shows the presence of defects, not their absence.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			There is no correlation here? This is the paradoxical nature of testing and the perception of testing. We will dive into each of the seven testing principles in the following weeks, and this is the first one relating to the value of testing.
		</pre>
		<main>
			This is the first principal listed in section 1.3 of the ISTQB foundational level syllabus. Testing reduces the likelihood of defects that are not found in a product. However, it does not mean that the product are both verified and validated 100% correctly, even if no defects remain. Most of the time, provided that proper testing processes were followed, those defects found remaining after all testing activities have been completed are usually lower in severity and priority. I have not seen any team that implemented a testing process that can find all the defects. It is not all doom and gloom; this is fine and realistic.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Therein lies caveat #1: some forms of verification and validation were executed. We are not going to go through what those are, since there are many layers of testing involved for different type of projects. One of the goals for testing is to proactively find and fix defects before changes are released or deployed.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			The other caveat #2: people have the right expectations. More importantly, we need to ensure stakeholders, such as leadership and customers, are aware that the team, to the best of their ability and given the contextual information they know at the time, developed and tested the feature requirements they have implemented. Despite all that, the end results still do not guarantee that there will not be defects, even if testing metrics tell a story otherwise.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Ultimately, there is some risk to all of this, but testing is still worth the effort to be successful in a project. Regardless of how much (limited or else) time the product engineering team can dedicate to testing, do it - it is better to be safe out there.
		</main>
	</topic>
	<topic isoValue="20241016">
		<link>verification-validation</link>
		<icon>bi bi-search</icon>
		<title>How do Verification and Validation Differ?</title>
		<date>October 16, 2024</date>
		<pre>
			I see "verify" and "validate" used strictly in certain ways or interchangeably during testing, is this a big deal? We will dig through the difference, application using an example, and some thoughts on their usages in this first installation of my new blog.
		</pre>
		<main>
			According to the glossary of ISTQB, they are defined as follows:<![CDATA[<br/>]]>
			Verification: confirmation by examination and through provision of objective evidence that specified requirements have been fulfilled.<![CDATA[<br/>]]>
			Validation: confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			At first glance, they look the same; but the main difference between the two is the end goal of the testing. We verify requirements of an application and we validate the intended use of the application. Let us imagine a form submission page. We verify all the fields are inputs and able to submit correctly. We validate that it meets the needs of the actual external and internal users: submission and data storage/application. These are strictly based on the definitions.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			There are many resources out there that claim certain activities as largely verification or validation testing. That is all fine because there are truths in there. However, to treat one or the other as simply as: static vs. dynamic, conducted before vs. after the development process, low-level vs. high-level testing, focusing on specs vs. products, before vs. after, white vs. black-box testing, which team is responsible for which, performed manually or by automation, requires code execution or not, are all beside the point.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Generally, testing activities are not rigid in most of SaaS. There are verification and validation testing occurring all the time during the course of planning and executing a project. This is why being agile in testing is also essential these days; product engineering teams are quickly iterating on new ideas every moment - they have to triple ensure they are delivering the correct feature requirements but also proving they are providing the right value for customers in everything they do, day in day out.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			In coming posts, I want to drive home the idea that software testing is messy, as with term definitions and how they are used in the real world. I mean that in a good way.
		</main>
	</topic>
</blog>