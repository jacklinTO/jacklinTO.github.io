<?xml version="1.0" encoding="UTF-8"?>
<blog>
	<topic isoValue="20241023">
		<link>testing-principle-1</link>
		<icon>bi bi-bug</icon>
		<title>Testing Principle #1 - Presence of Defects</title>
		<date>October 23, 2024</date>
		<pre>
			Testing shows the presence of defects, not their absence.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			There is no correlation here? This is the paradoxical nature of testing and the perception of testing. We will dive into each of the seven testing principles in the following weeks, and this is the first one relating to the value of testing.
		</pre>
		<main>
			This is the first principal listed in section 1.3 of the ISTQB foundational level syllabus. Testing reduces the likelihood of defects that are not found in a product. However, it does not mean that the product are both verified and validated 100% correctly, even if no defects remain. Most of the time, provided that proper testing processes were followed, those defects found remaining after all testing activities have been completed are usually lower in severity and priority. I have not seen any team that implemented a testing process that can find all the defects. It is not all doom and gloom; this is fine and realistic.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Therein lies caveat #1: some forms of verification and validation were executed. We are not going to go through what those are, since there are many layers of testing involved for different type of projects. One of the goals for testing is to proactively find and fix defects before changes are released or deployed.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			The other caveat #2: people have the right expectations. More importantly, we need to ensure stakeholders, such as leadership and customers, are aware that the team, to the best of their ability and given the contextual information they know at the time, developed and tested the feature requirements they have implemented. Despite all that, the end results still do not guarantee that there will not be defects, even if testing metrics tell a story otherwise.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Ultimately, there is some risk to all of this, but testing is still worth the effort to be successful in a project. Regardless of how much (limited or else) time the product engineering team can dedicate to testing, do it - it is better to be safe out there.
		</main>
	</topic>
	<topic isoValue="20241016">
		<link>verification-validation</link>
		<icon>bi bi-search</icon>
		<title>How do Verification and Validation Differ?</title>
		<date>October 16, 2024</date>
		<pre>
			I see "verify" and "validate" used strictly in certain ways or interchangeably during testing, is this a big deal? We will dig through the difference, application using an example, and some thoughts on their usages in this first installation of my new blog.
		</pre>
		<main>
			According to the glossary of ISTQB, they are defined as follows:<![CDATA[<br/>]]>
			Verification: confirmation by examination and through provision of objective evidence that specified requirements have been fulfilled.<![CDATA[<br/>]]>
			Validation: confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			At first glance, they look the same; but the main difference between the two is the end goal of the testing. We verify requirements of an application and we validate the intended use of the application. Let us imagine a form submission page. We verify all the fields are inputs and able to submit correctly. We validate that it meets the needs of the actual external and internal users: submission and data storage/application. These are strictly based on the definitions.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			There are many resources out there that claim certain activities as largely verification or validation testing. That is all fine because there are truths in there. However, to treat one or the other as simply as: static vs. dynamic, conducted before vs. after the development process, low-level vs. high-level testing, focusing on specs vs. products, before vs. after, white vs. black-box testing, which team is responsible for which, performed manually or by automation, requires code execution or not, are all beside the point.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Generally, testing activities are not rigid in most of SaaS. There are verification and validation testing occurring all the time during the course of planning and executing a project. This is why being agile in testing is also essential these days; product engineering teams are quickly iterating on new ideas every moment - they have to triple ensure they are delivering the correct feature requirements but also proving they are providing the right value for customers in everything they do, day in day out.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			In coming posts, I want to drive home the idea that software testing is messy, as with term definitions and how they are used in the real world. I mean that in a good way.
		</main>
	</topic>
</blog>