<?xml version="1.0" encoding="UTF-8"?>
<blog>
	<topic isoValue="20241120">
		<link>testing-principle-4</link>
		<icon>bi bi-repeat</icon>
		<title>Testin Principle #5 - Beware of the pesticide paradox</title>
		<date>November 20, 2024</date>
		<pre>
			The continuous misuse of the same pesticides or antibiotics make them resistant to infections they are supposed to fighting against. The same can be said for executing the same tests repeatedly. We will examine what we can do about it in this next installment of the seven testing principles.
		</pre>
		<main>
			Continually running the same tests can prove to be futile because we may not find new bugs. In particular, remember that our first principle (check that previous post) still applies; our expectations cannot be too optimistic when we repeat ourselves. This is not to say we should not run them - we definitely should, but with some twists.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Updating test cases periodically is important. This means setting up a frequency by which test cases are reviewed and updated with the most current behaviours and information. Furthermore, when new features are developed, updating existing test cases should be the default instead of creating new ones. This is essential to keep tests to a manageable state.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Another good way to keep tests fresh is to have different testers on the team execute them. This means setting up a frequency by which test case executions are rotated among team members. On one hand, since everyone thinks differently, how test steps are executed can vary, and therefore, tests can be updated accordingly. On the other hand, it facilitates knowledge sharing and upskills the team to be subject matter experts.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Lastly, implementing test automation can be a good investment if these tests need to be run consistently, like regression test cases. Sometimes, new bugs can even appear when tests are run repeatedly this way. In the end, it is not enough to just have tests - we must be able to refresh them whenever there is a need, since the platform under testing is never static.
		</main>
	</topic>
	<topic isoValue="20241113">
		<link>testing-principle-4</link>
		<icon>bi bi-arrows-angle-contract</icon>
		<title>Testing Principle #4 - Defects cluster together</title>
		<date>November 13, 2024</date>
		<pre>
			Defects cluster together<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Bugs in real life and in software are social creatures; they usually congregate in the same small (tight, dark) crevices of buildings and system modules. We shall dig deeper into this phenomenon in the fourth installment of the seven testing principles.
		</pre>
		<main>
			This is the fourth principal listed in Section 1.3 of the ISTQB foundational level syllabus. Typically, defects are found during testing in small clusters of the platform. This runs counter to most statistical models out there, with even probability outcomes, because software development is a whole different animal. Defects appear not randomly but are distributed across certain modules in the platform since they are not alike.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Understandably, modules in a system are not all treated the same. Some are simple because they are static and have trivial workflows. While others are complex because they depend on widely disparate services, many developers worked on them, and have complicated business logics. The modules in the latter category take more time to plan and develop which have a higher likelihood of failures. Conversely, sometimes modules in the former category may have less attention devoted to them during planning and development or be executed more quickly with less oversight and inexperienced personnel. Finally, mission-critical functionalities can attract more interest. There are other factors that can contribute to where defects are found since dealing with software is always messy.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			This is why the previously mentioned risk analysis is important to assess which modules are worth more testing focus. That task takes the probabities out of verification and validation so that we can take an analytical approach in determining which and when certain test cases are executed for each risk area.
		</main>
	</topic>
	<topic isoValue="20241106">
		<link>testing-principle-3</link>
		<icon>bi bi-arrow-left-circle</icon>
		<title>Testing Principle #3 - Early testing saves time and money</title>
		<date>November 6, 2024</date>
		<pre>
			Early testing saves time and money.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			This is pretty intuitive and is agreed upon by everyone as a sound testing strategy. Executing some forms of testing earlier in the software development lifecycle is also called "shift left". We will look closer at this concept and understand the degree of ease in putting this into practice.
		</pre>
		<main>
			This is the third principal listed in Section 1.3 of the ISTQB foundational level syllabus. In order to have effective testing (finding defects, for example) in a project, some forms of verification and validation need to be executed as early as possible in the software development lifecycle. When viewing a typical diagram of the SDLC diagram, reading from left to right, it means shifting testing more towards the left direction, "shift left". This is especially valuable in reducing the length and the cost of the project. The cost of fixing issues is typically lower during the planning and development phases because the stakeholders are actively working on it already. The earlier defects are found and resolved, the earlier we can arrive at a stable version of the platform for release.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			As I mentioned earlier in a previous post about verification and validation testing, they can occur at all phases of a project or SDLC. The emphasis is on planning (and also being flexible enough) to apply the right approaches at the appropriate time to be save time and money properly. It can include requirement reviews during planning, static scans during development, and functional testing against requirements implemented, etc. This is called "continuous testing", a larger topic we can revisit more in a later post.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Testing early is something we all strive to achieve in a perfect world. However, due to budget and deadline constraints, that is not always possible. So we need to work with stakeholders to communicate the various testing strategies, however much that can be applied, used at each step of the way. Given the amount of resources, we may encounter that certain testing coverage may not be achieved or even be able to be executed until later. This means specific types of defects may not be uncovered until another dependency is done. That realistic perspective of testing is important to the success of a project.
		</main>
	</topic>
	<topic isoValue="20241030">
		<link>testing-principle-2</link>
		<icon>bi bi-bicycle</icon>
		<title>Testing Principle #2 - Exhaustive Testing is Impossible</title>
		<date>October 30, 2024</date>
		<pre>
			Exhaustive testing is impossible.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Nor do we want to exhaust ourselves executing an infinite number of test scenarios. There are more valuable ways to focus our testing efforts. We shall explore some of them in this second installment of the seven testing principles.
		</pre>
		<main>
			This is the second principal listed in Section 1.3 of the ISTQB foundational level syllabus. Testing every combination of inputs and outputs for any platform is not realistic except for trivial cases. That number could be infinite or close to it. If we have unlimited resources and time, we can come close to achieving total coverage. Since there is always an imminent deadline ahead, we should pick those test scenarios by focusing on risk analysis, test techniques, and priorities.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Risk analysis involves stakeholders identifying risk items in the platform and assessing their risk levels. Based on this analysis, we can determine which and when certain test cases are executed for each risk item. This is valuable because each business can determine their own levels of exposure to risks and test accordingly.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Test techniques mean some forms of verification and validation to execute. As mentioned before, the many layers of testing, when chosen appropriately, can greatly optimize testing for teams.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Priorities can be established by different factors: requirements and time. Based on the priority of user requirements, we can prioritize which and when certain test cases are executed for each specification. This is valuable because we are testing from the perspective and the importance of each end user scenario. The concept of time will be expanded on more in the next post.
		</main>
	</topic>
	<topic isoValue="20241023">
		<link>testing-principle-1</link>
		<icon>bi bi-bug</icon>
		<title>Testing Principle #1 - Presence of Defects</title>
		<date>October 23, 2024</date>
		<pre>
			Testing shows the presence of defects, not their absence.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			There is no correlation here? This is the paradoxical nature of testing and the perception of testing. We will dive into each of the seven testing principles in the following weeks, and this is the first one relating to the value of testing.
		</pre>
		<main>
			This is the first principal listed in Section 1.3 of the ISTQB foundational level syllabus. Testing reduces the likelihood of defects that are not found in a product. However, it does not mean that the product are both verified and validated 100% correctly, even if no defects remain. Most of the time, provided that proper testing processes were followed, those defects found remaining after all testing activities have been completed are usually lower in severity and priority. I have not seen any team that implemented a testing process that can find all the defects. It is not all doom and gloom; this is fine and realistic.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Therein lies caveat #1: some forms of verification and validation were executed. We are not going to go through what those are, since there are many layers of testing involved for different type of projects. One of the goals for testing is to proactively find and fix defects before changes are released or deployed.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			The other caveat #2: people have the right expectations. More importantly, we need to ensure stakeholders, such as leadership and customers, are aware that the team, to the best of their ability and given the contextual information they know at the time, developed and tested the feature requirements they have implemented. Despite all that, the end results still do not guarantee that there will not be defects, even if testing metrics tell a story otherwise.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Ultimately, there is some risk to all of this, but testing is still worth the effort to be successful in a project. Regardless of how much (limited or else) time the product engineering team can dedicate to testing, do it - it is better to be safe out there.
		</main>
	</topic>
	<topic isoValue="20241016">
		<link>verification-validation</link>
		<icon>bi bi-search</icon>
		<title>How do Verification and Validation Differ?</title>
		<date>October 16, 2024</date>
		<pre>
			I see "verify" and "validate" used strictly in certain ways or interchangeably during testing, is this a big deal? We will dig through the difference, application using an example, and some thoughts on their usages in this first installation of my new blog.
		</pre>
		<main>
			According to the glossary of ISTQB, they are defined as follows:<![CDATA[<br/>]]>
			Verification: confirmation by examination and through provision of objective evidence that specified requirements have been fulfilled.<![CDATA[<br/>]]>
			Validation: confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			At first glance, they look the same; but the main difference between the two is the end goal of the testing. We verify requirements of an application and we validate the intended use of the application. Let us imagine a form submission page. We verify all the fields are inputs and able to submit correctly. We validate that it meets the needs of the actual external and internal users: submission and data storage/application. These are strictly based on the definitions.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			There are many resources out there that claim certain activities as largely verification or validation testing. That is all fine because there are truths in there. However, to treat one or the other as simply as: static vs. dynamic, conducted before vs. after the development process, low-level vs. high-level testing, focusing on specs vs. products, white vs. black-box testing, which team is responsible for which, performed manually or by automation, requires code execution or not, are all beside the point.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			Generally, testing activities are not rigid in most of SaaS. There are verification and validation testing occurring all the time during the course of planning and executing a project. This is why being agile in testing is also essential these days; product engineering teams are quickly iterating on new ideas every moment - they have to triple ensure they are delivering the correct feature requirements but also proving they are providing the right value for customers in everything they do, day in day out.<![CDATA[<br/>]]>
			<![CDATA[<br/>]]>
			In coming posts, I want to drive home the idea that software testing is messy, as with term definitions and how they are used in the real world. I mean that in a good way.
		</main>
	</topic>
</blog>